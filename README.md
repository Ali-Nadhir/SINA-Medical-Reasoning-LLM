🧠 SINA Medical Reasoning LLM (Fine-Tuned by Ali Nadhir)
🚀 Model Overview
SINA Medical Reasoning LLM is a fine-tuned version of Qwen2.5-7B, developed specifically for advanced medical diagnostic reasoning. By incorporating Chain of Thought (CoT) methodologies, the model is designed to perform step-by-step clinical analysis and decision-making.

Inspired by the legacy of Ibn Sina (Avicenna), this model merges classical medical reasoning principles with the capabilities of large language models.

🔧 Fine-Tuning Details
Base Model: Qwen2.5-7B

Fine-tuned by: Ali Nadhir

Library: Hugging Face Transformers using Unsloth

Dataset: FreedomIntelligence/medical-o1-reasoning-SFT

Hardware: 1× NVIDIA A100 (40GB VRAM)

Objective: Equip a non-reasoning LLM with structured medical reasoning capabilities to support accurate and explainable clinical inference.

📈 Reasoning Capabilities & Use Cases
Fine-tuning with Chain of Thought examples from a medical domain enables the model to improve performance in:

🩺 Differential diagnosis and symptom analysis

🧠 Multi-step clinical reasoning and logic chaining

📋 Structured Q&A in medical consultations

🧩 Educational simulations and AI-assisted diagnosis

Ideal for:

Clinical AI prototypes

Healthcare research and experimentation

MedEd (Medical Education) tools

Diagnostic reasoning assistants

🔐 Note: This release includes only the 4-bit quantized (4Q) version of the model, optimized for resource-constrained deployment.
Note: the model file on huggingface 
